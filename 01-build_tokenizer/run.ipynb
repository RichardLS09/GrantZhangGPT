{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "! pip install tokenizers\n",
    "! pip install sentencepiece\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练一个自己的tokenizer（走通流程）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备训练tokenizer的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "# 准备所有的语料\n",
    "# 应该是按照某些比例做语种、cate的采样，这里为了简单，就是中文、英文各若干个。\n",
    "\n",
    "sample_size = 10000\n",
    "TMP_FILE = \"tmp_file.txt\"\n",
    "\n",
    "# 英语\n",
    "dataset_en = datasets.load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train+test+validation\")\n",
    "# 中文\n",
    "dataset_cn = datasets.load_dataset(\"m-a-p/MAP-CC\", split='train', streaming=True)\n",
    "\n",
    "all_text = []\n",
    "for index, d in enumerate(dataset_en, 1):\n",
    "    if index > sample_size:\n",
    "        break\n",
    "    all_text.append(d[\"text\"])\n",
    "for index, d in enumerate(dataset_cn, 1):\n",
    "    if index > sample_size:\n",
    "        break\n",
    "    all_text.append(d[\"text\"])\n",
    "\n",
    "with open(TMP_FILE, \"w\") as f:\n",
    "    for text in all_text:\n",
    "        # 去除首尾的空字符\n",
    "        print(text.strip(), file=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用sentencepiece进行tokenizer的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "                                input=TMP_FILE,\n",
    "                                model_prefix=\"spm_model\",\n",
    "                                model_type=\"bpe\",\n",
    "                                vocab_size=10000,\n",
    "                                self_test_sample_size=0,\n",
    "                                input_format=\"text\",\n",
    "                                character_coverage=1.0,\n",
    "                                num_threads=os.cpu_count(),\n",
    "                                split_digits=True,\n",
    "                                allow_whitespace_only_pieces=True,\n",
    "                                byte_fallback=True,\n",
    "                                normalization_rule_name=\"identity\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用tokenizers进行tokenizer的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, pre_tokenizers, models, trainers\n",
    "from tokenizers.pre_tokenizers import Digits, Whitespace\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "# 不做normalizion\n",
    "# tokenizer.normalizer = normalizer\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=10000,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "    special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"],\n",
    ")\n",
    "\n",
    "tokenizer.train([TMP_FILE], trainer=trainer)\n",
    "tokenizer.save(\"tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些总结\n",
    "\n",
    "1. 训练一个多语的tokenizer时候，需要把所有语种、每个语种领域（丰富性）都有准备，同时按照占比进行对齐。\n",
    "2. 通过观察目前开源模型的tokenzier，可以发现：\n",
    "   - vocab_size一般订一个大概范围（根据已有的实验/其他tokenizer），然后再根据训练时分布式训练的并行考虑，尽量保证。\n",
    "   - 对于标点符号、HTML中的tag、预保留的token都是通过user_defined_symbols预先定义的（baichuan-inc/Baichuan-7B中前几百个字符可以观察出）。\n",
    "   - 一般要设置将数字都进行切分，这样对于数学能力有提升。——引申出来，是否可以将latex、markdown的一些符号也进行切分，这样对这些的学习也会更好？\n",
    "3. 自己训练一个好的tokenizer是比较难的，可以使用开源的+自己领域的进行增量训练。"
   ]
  }
 ],
 "metadata": {
  "fileId": "09b86692-6411-4a7d-83d6-97b5bc1970d4",
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
